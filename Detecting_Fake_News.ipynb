{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Detecting Fake News",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpdhanam/Jaya-priya-J-.devtern-fake-news-detection/blob/main/Detecting_Fake_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'textdb3:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F129603%2F310019%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240617%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240617T173046Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D020b7ef421f419c3fcdcf719f82f79122cf4f580b6db6172a4c54fad9ff815d0501ec0afbb243ca683b5236a6a1c884c672f18afcd0e00acf50519165bc40773ee37c4c16175f4b8b41d92de5e3aef00559213417b99ffa0480d4f0cee45fdb852af6b5c3c2ee311bc7c5c9b49ca7f5271dfdaa2e50e51e2f61a3ac18df02040faeb14acf803063eabd529915237369bd98ef040775ff4bcf3e82cf3a042a3e4acced9fae939a2782677df52fc69468c5f2c2b9ce70d445836d6458e59159372c90be15430a78e521ce3804888bcbccffd852f35170da3e557171a8bc6f11b76a7c2ecb9e4348fa20174e898654df91c0aec68711142bc18ae72e5a3ce96f135'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "C4g1p5rUQF8K"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make necessary imports"
      ],
      "metadata": {
        "id": "fJlPwo8WQF8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:37.389287Z",
          "iopub.execute_input": "2021-07-05T06:17:37.38975Z",
          "iopub.status.idle": "2021-07-05T06:17:38.662257Z",
          "shell.execute_reply.started": "2021-07-05T06:17:37.389667Z",
          "shell.execute_reply": "2021-07-05T06:17:38.661172Z"
        },
        "trusted": true,
        "id": "j4ETA8RkQF8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now, let’s read the data into a DataFrame, and get the shape of the data and the first 5 records."
      ],
      "metadata": {
        "id": "zztyZtWqQF8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the data\n",
        "df=pd.read_csv('../input/textdb3/fake_or_real_news.csv')\n",
        "\n",
        "#Get shape and head\n",
        "df.shape\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:38.664564Z",
          "iopub.execute_input": "2021-07-05T06:17:38.665038Z",
          "iopub.status.idle": "2021-07-05T06:17:39.625359Z",
          "shell.execute_reply.started": "2021-07-05T06:17:38.664973Z",
          "shell.execute_reply": "2021-07-05T06:17:39.624266Z"
        },
        "trusted": true,
        "id": "1A3mPCFzQF8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And get the labels from the DataFrame"
      ],
      "metadata": {
        "id": "AW1DdDV6QF8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Get the labels\n",
        "labels=df.label\n",
        "labels.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:39.626968Z",
          "iopub.execute_input": "2021-07-05T06:17:39.62734Z",
          "iopub.status.idle": "2021-07-05T06:17:39.63704Z",
          "shell.execute_reply.started": "2021-07-05T06:17:39.627298Z",
          "shell.execute_reply": "2021-07-05T06:17:39.635857Z"
        },
        "trusted": true,
        "id": "DPjAnuSaQF8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the dataset into training and testing sets."
      ],
      "metadata": {
        "id": "al4mdxZhQF8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Split the dataset\n",
        "x_train,x_test,y_train,y_test=train_test_split(df['text'], labels, test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:39.638604Z",
          "iopub.execute_input": "2021-07-05T06:17:39.638941Z",
          "iopub.status.idle": "2021-07-05T06:17:39.658364Z",
          "shell.execute_reply.started": "2021-07-05T06:17:39.638898Z",
          "shell.execute_reply": "2021-07-05T06:17:39.656601Z"
        },
        "trusted": true,
        "id": "UNn3citOQF8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now, fit and transform the vectorizer on the train set, and transform the vectorizer on the test set."
      ],
      "metadata": {
        "id": "ApFfML_OQF8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##DataFlair - Initialize a TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df =0.9)\n",
        "## DataFlair - fit and transform train set, transform test set\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(x_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:39.661769Z",
          "iopub.execute_input": "2021-07-05T06:17:39.662363Z",
          "iopub.status.idle": "2021-07-05T06:17:44.929788Z",
          "shell.execute_reply.started": "2021-07-05T06:17:39.662308Z",
          "shell.execute_reply": "2021-07-05T06:17:44.928918Z"
        },
        "trusted": true,
        "id": "GmOs4MZgQF8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Then, we’ll predict on the test set from the TfidfVectorizer and calculate the accuracy with accuracy_score() from sklearn.metrics."
      ],
      "metadata": {
        "id": "YIcZYtDLQF8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Initialize a PassiveAggressiveClassifier\n",
        "pac=PassiveAggressiveClassifier(max_iter=50)\n",
        "pac.fit(tfidf_train,y_train)\n",
        "\n",
        "#DataFlair - Predict on the test set and calculate accuracy\n",
        "y_pred=pac.predict(tfidf_test)\n",
        "score=accuracy_score(y_test,y_pred)\n",
        "print(f'Accuracy: {round(score*100,2)}%')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:44.930891Z",
          "iopub.execute_input": "2021-07-05T06:17:44.931348Z",
          "iopub.status.idle": "2021-07-05T06:17:45.082723Z",
          "shell.execute_reply.started": "2021-07-05T06:17:44.931316Z",
          "shell.execute_reply": "2021-07-05T06:17:45.081842Z"
        },
        "trusted": true,
        "id": "HGwkLUWiQF8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got an accuracy of 92.82% with this model. Finally, let’s print out a confusion matrix to gain insight into the number of false and true negatives and positives."
      ],
      "metadata": {
        "id": "qeCBQfYlQF8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DataFlair - Build confusion matrix\n",
        "confusion_matrix(y_test,y_pred, labels=['FAKE','REAL'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-05T06:17:45.083709Z",
          "iopub.execute_input": "2021-07-05T06:17:45.084109Z",
          "iopub.status.idle": "2021-07-05T06:17:45.096613Z",
          "shell.execute_reply.started": "2021-07-05T06:17:45.084079Z",
          "shell.execute_reply": "2021-07-05T06:17:45.095545Z"
        },
        "trusted": true,
        "id": "GtXRKLhCQF8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary\n",
        "\n",
        "Today, we learned to detect fake news with Python. We took a political dataset, implemented a TfidfVectorizer, initialized a PassiveAggressiveClassifier, and fit our model. We ended up obtaining an accuracy of 92.82% in magnitude.\n",
        "\n",
        "Hope you enjoyed the fake news detection python project. Keep visiting DataFlair for more interesting python, data science, and machine learning projects."
      ],
      "metadata": {
        "id": "ICtaaKtDQF8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to DataFlair I learn a lot from it."
      ],
      "metadata": {
        "id": "Rl-pVqJrQF8l"
      }
    }
  ]
}